{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["^C\n","Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stdout","output_type":"stream","text":["Collecting datasets==1.1.2\n","  Downloading datasets-1.1.2-py3-none-any.whl (147 kB)\n","     ------------------------------------- 147.1/147.1 kB 35.3 kB/s eta 0:00:00\n","Collecting pytorch_lightning==1.0.3\n","  Downloading pytorch_lightning-1.0.3-py3-none-any.whl (533 kB)\n","     ------------------------------------- 533.9/533.9 kB 33.3 kB/s eta 0:00:00\n","Collecting wandb==0.10.8\n","  Downloading wandb-0.10.8-py2.py3-none-any.whl (1.7 MB)\n","     ---------------------------------------- 1.7/1.7 MB 14.1 kB/s eta 0:00:00\n","Collecting transformers==3.4.0\n","  Downloading transformers-3.4.0-py3-none-any.whl (1.3 MB)\n","     ---------------------------------------- 1.3/1.3 MB 28.2 kB/s eta 0:00:00\n","Collecting xxhash\n","  Downloading xxhash-3.4.1-cp310-cp310-win_amd64.whl (29 kB)\n","Collecting filelock\n","  Downloading filelock-3.12.4-py3-none-any.whl (11 kB)\n","Requirement already satisfied: numpy>=1.17 in c:\\python\\lib\\site-packages (from datasets==1.1.2) (1.23.1)\n","Collecting pyarrow>=0.17.1\n","  Downloading pyarrow-13.0.0-cp310-cp310-win_amd64.whl (24.3 MB)\n","     ----------------------                  14.0/24.3 MB 27.2 kB/s eta 0:06:20\n"]},{"name":"stderr","output_type":"stream","text":["ERROR: Exception:\n","Traceback (most recent call last):\n","  File \"c:\\python\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 437, in _error_catcher\n","    yield\n","  File \"c:\\python\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 560, in read\n","    data = self._fp_read(amt) if not fp_closed else b\"\"\n","  File \"c:\\python\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 526, in _fp_read\n","    return self._fp.read(amt) if amt is not None else self._fp.read()\n","  File \"c:\\python\\lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 90, in read\n","    data = self.__fp.read(amt)\n","  File \"c:\\python\\lib\\http\\client.py\", line 464, in read\n","    s = self.fp.read(amt)\n","  File \"c:\\python\\lib\\socket.py\", line 705, in readinto\n","    return self._sock.recv_into(b)\n","  File \"c:\\python\\lib\\ssl.py\", line 1273, in recv_into\n","    return self.read(nbytes, buffer)\n","  File \"c:\\python\\lib\\ssl.py\", line 1129, in read\n","    return self._sslobj.read(len, buffer)\n","TimeoutError: The read operation timed out\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"c:\\python\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 160, in exc_logging_wrapper\n","    status = run_func(*args)\n","  File \"c:\\python\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 247, in wrapper\n","    return func(self, options, args)\n","  File \"c:\\python\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 400, in run\n","    requirement_set = resolver.resolve(\n","  File \"c:\\python\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 92, in resolve\n","    result = self._result = resolver.resolve(\n","  File \"c:\\python\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 481, in resolve\n","    state = resolution.resolve(requirements, max_rounds=max_rounds)\n","  File \"c:\\python\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 373, in resolve\n","    failure_causes = self._attempt_to_pin_criterion(name)\n","  File \"c:\\python\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 213, in _attempt_to_pin_criterion\n","    criteria = self._get_updated_criteria(candidate)\n","  File \"c:\\python\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 204, in _get_updated_criteria\n","    self._add_to_criteria(criteria, requirement, parent=candidate)\n","  File \"c:\\python\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 172, in _add_to_criteria\n","    if not criterion.candidates:\n","  File \"c:\\python\\lib\\site-packages\\pip\\_vendor\\resolvelib\\structs.py\", line 151, in __bool__\n","    return bool(self._sequence)\n","  File \"c:\\python\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 155, in __bool__\n","    return any(self)\n","  File \"c:\\python\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 143, in <genexpr>\n","    return (c for c in iterator if id(c) not in self._incompatible_ids)\n","  File \"c:\\python\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 47, in _iter_built\n","    candidate = func()\n","  File \"c:\\python\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 206, in _make_candidate_from_link\n","    self._link_candidate_cache[link] = LinkCandidate(\n","  File \"c:\\python\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 297, in __init__\n","    super().__init__(\n","  File \"c:\\python\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 162, in __init__\n","    self.dist = self._prepare()\n","  File \"c:\\python\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 231, in _prepare\n","    dist = self._prepare_distribution()\n","  File \"c:\\python\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 308, in _prepare_distribution\n","    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n","  File \"c:\\python\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 491, in prepare_linked_requirement\n","    return self._prepare_linked_requirement(req, parallel_builds)\n","  File \"c:\\python\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 536, in _prepare_linked_requirement\n","    local_file = unpack_url(\n","  File \"c:\\python\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 166, in unpack_url\n","    file = get_http_url(\n","  File \"c:\\python\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 107, in get_http_url\n","    from_path, content_type = download(link, temp_dir.path)\n","  File \"c:\\python\\lib\\site-packages\\pip\\_internal\\network\\download.py\", line 147, in __call__\n","    for chunk in chunks:\n","  File \"c:\\python\\lib\\site-packages\\pip\\_internal\\cli\\progress_bars.py\", line 53, in _rich_progress_bar\n","    for chunk in iterable:\n","  File \"c:\\python\\lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 63, in response_chunks\n","    for chunk in response.raw.stream(\n","  File \"c:\\python\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 621, in stream\n","    data = self.read(amt=amt, decode_content=decode_content)\n","  File \"c:\\python\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 559, in read\n","    with self._error_catcher():\n","  File \"c:\\python\\lib\\contextlib.py\", line 153, in __exit__\n","    self.gen.throw(typ, value, traceback)\n","  File \"c:\\python\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 442, in _error_catcher\n","    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n","pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\n","\n","[notice] A new release of pip available: 22.3.1 -> 23.2.1\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]}],"source":["## install dependencies\n","%pip install datasets==1.1.2 pytorch_lightning==1.0.3 wandb==0.10.8 transformers==3.4.0"]},{"cell_type":"markdown","metadata":{},"source":["## 0. Dependencies"]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'torch'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgc\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtqdm\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m# data\u001b[39;00m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"]}],"source":["# utils \n","import os\n","import gc\n","import tqdm\n","import torch\n","import pandas as pd\n","\n","# data\n","from transformers import AutoTokenizer\n","from datasets import load_dataset\n","from torch.utils.data import random_split, Dataset, DataLoader\n","\n","\n","\n","# model\n","from transformers import AutoModel\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","\n","# training and evaluation \n","import wandb\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import EarlyStopping, ProgressBar, ModelCheckpoint\n","from pytorch_lightning.loggers import WandbLogger\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_auc_score, roc_curve\n","import seaborn as sns\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# device  \n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# seed\n","torch.manual_seed(42)\n"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Custom Dataset "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class NewsDataset(Dataset):\n","    \"Custom Dataset class to create the torch dataset\"\n","    \n","    def __init__(self, root_dir, tokenizer,  max_len=128):\n","        \"\"\"\n","            root_dir: path where data is residing\n","            tokenizer: tokenizer will be used to tokenize the text\n","            max_len: max_len for text, padding/trimming will be applied to follow this rule\n","        \"\"\"\n","        \n","        self.tokenizer = tokenizer\n","        \n","        self.data = load_dataset(\"csv\", data_files=[os.path.join(root_dir, \"Fake.csv\"), os.path.join(root_dir, \"True.csv\")])['train']\n","        \n","        \n","        self.text = self.data['title']\n","        self.label = self.data['label']\n","                        \n","        self.max_len = max_len\n","                                        \n","        \n","        \n","    def __len__(self):\n","        \"__len__ function returns the size of the data =\"\n","        return len(self.text)\n","    \n","    def __getitem__(self, idx):\n","        \"\"\"\n","            idx: index of the data to retrieve\n","\n","            returns: A dictionary containing input ids based on tokenizer's vocabulary, attention mask and label tensors \n","        \"\"\"\n","        \n","        text = self.text[idx]\n","        label = self.label[idx]\n","        \n","        input_encoding = self.tokenizer.encode_plus(\n","            text=text,\n","            truncation=True,\n","            max_length=self.max_len,\n","            return_tensors=\"pt\",\n","            return_attention_mask=True,\n","            padding=\"max_length\",\n","        )\n","        \n","        return {\n","            \"input_ids\":input_encoding['input_ids'].squeeze(),\n","            \"attention_mask\":input_encoding['attention_mask'].squeeze(),\n","            \"label\":torch.tensor([label], dtype=torch.float)\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## 2. Model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class Model(nn.Module):\n","\n","    \"\"\" \n","        Fake News Classifier Model\n","        A pretrained model is used as for contextualized embedding and a classifier on top of that. \n","    \n","    \"\"\"\n","\n","    def __init__(self, model_name, num_classes=2):\n","        \"\"\"\n","            model_name:  What base model to use from hugginface transformers\n","            num_classes: Number of classes to classify. This is simple binary classification hence 2 classes\n","        \"\"\"\n","        super().__init__()\n","        \n","\n","        # pretrained transformer model as base\n","        self.base = AutoModel.from_pretrained(pretrained_model_name_or_path=model_name)\n","\n","\n","        # nn classifier on top of base model\n","        self.classfier = nn.Sequential(*[\n","            nn.Linear(in_features=768, out_features=256),\n","            nn.LeakyReLU(),\n","            nn.Linear(in_features=256, out_features=num_classes),\n","            nn.Sigmoid()\n","        ])\n","\n","\n","    def forward(self, input_ids, attention_mask=None):\n","        \"\"\"\n","            input_ids: input ids tensors for tokens  shape = [batch_size, max_len]\n","            attention_mask: attention for input ids, 0 for pad tokens and 1 for non-pad tokens [batch_size, max_len]\n","\n","            returns: logits tensors as output, shape = [batch, num_classes]\n","\n","        \"\"\"\n","\n","\n","        outputs = self.base(input_ids=input_ids, attention_mask=attention_mask)\n","        \n","        pooler = outputs[1]\n","        # pooler.shape = [batch_size, hidden_size]\n","\n","        logits = self.classfier(pooler)\n","\n","\n","        return logits\n"]},{"cell_type":"markdown","metadata":{},"source":["## 3. PyTorchLightning Data and Trainer Module "]},{"cell_type":"markdown","metadata":{},"source":["#### Data Module"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class FakeNewsDataModule(pl.LightningDataModule):\n","\n","    \"\"\"Lightning Data Module to detach data from model\"\"\"\n","\n","    def __init__(self, config):\n","\n","        \"\"\"\n","            config: a dicitonary containing data configuration such as batch size, split_size etc\n","        \"\"\"\n","        super().__init__()\n","\n","        self.config = config\n","\n","\n","        # prepare and setup the dataset\n","        self.prepare_data()\n","        self.setup()\n","\n","    def prepare_data(self):\n","        \"\"\"prepare datset\"\"\"\n","\n","        tokenizer = AutoTokenizer.from_pretrained(self.config['model_name'])\n","\n","        self.dataset = NewsDataset(root_dir=self.config['root_dir'], tokenizer=tokenizer, max_len=self.config['max_len'])\n","\n","\n","\n","    def setup(self):\n","        \"\"\"make assignments here (val/train/test split)\"\"\"\n","        \n","        train_size = self.config['train_size']\n","\n","        lengths = [int(len(self.dataset)*train_size), len(self.dataset)-int(len(self.dataset)*train_size)]\n","\n","        self.train_datset, self.test_dataset =  random_split(dataset=self.dataset, lengths=lengths)\n","\n","\n","    def train_dataloader(self):\n","\n","        return DataLoader(dataset=self.train_datset, batch_size=self.config['batch_size'], shuffle=True, num_workers=self.config['num_workers'])\n","\n","    def val_dataloader(self):\n","        return DataLoader(dataset=self.test_dataset, batch_size=self.config['batch_size'], shuffle=False, num_workers=self.config['num_workers'])\n","\n","    def test_dataloader(self):\n","        # same as validation data\n","        return DataLoader(dataset=self.test_dataset, batch_size=self.config['batch_size'], shuffle=False, num_workers=self.config['num_workers'])"]},{"cell_type":"markdown","metadata":{},"source":["#### Trainer Module"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class LightningModel(pl.LightningModule):\n","\n","    \"\"\"\n","        LightningModel as trainer model\n","    \"\"\"\n","    \n","    def __init__(self, config):\n","        \"\"\"\n","            config: training and other conifguration\n","        \"\"\"\n","\n","        super(LightningModel, self).__init__()\n","        \n","        self.config = config\n","        \n","        self.model = Model(model_name=self.config['model_name'], num_classes=self.config['num_classes'])\n","\n","        \n","    def forward(self, input_ids, attention_mask=None):\n","        logits  = self.model(input_ids=input_ids, attention_mask=attention_mask)\n","        return logits.squeeze()\n","    \n","    def configure_optimizers(self):\n","        return optim.AdamW(params=self.parameters(), lr=self.config['lr'])\n","  \n","    \n","    def training_step(self, batch, batch_idx):\n","        \n","        input_ids, attention_mask, targets = batch['input_ids'], batch['attention_mask'], batch['label'].squeeze()\n","        logits = self(input_ids=input_ids, attention_mask=attention_mask)\n","        loss = F.mse_loss(logits, targets)\n","        \n","        pred_labels = logits.cpu() > 0.5 # logits.argmax(dim=1).cpu() for non-sigmoid\n","        acc = accuracy_score(targets.cpu(), pred_labels)\n","        f1 = f1_score(targets.cpu(), pred_labels, average=self.config['average'])\n","        wandb.log({\"loss\":loss, \"accuracy\":acc, \"f1_score\":f1})\n","        return {\"loss\":loss, \"accuracy\":acc, \"f1_score\":f1}\n","\n","    \n","    def validation_step(self, batch, batch_idx):\n","        input_ids, attention_mask, targets = batch['input_ids'], batch['attention_mask'], batch['label'].squeeze()\n","        logits = self(input_ids=input_ids, attention_mask=attention_mask)\n","        loss = F.mse_loss(logits, targets)\n","        pred_labels = logits.cpu() > 0.5 # logits.argmax(dim=1).cpu() for non-sigmoid\n","        acc = accuracy_score(targets.cpu(), pred_labels)\n","        f1 = f1_score(targets.cpu(), pred_labels, average=self.config['average'])\n","        precision = precision_score(targets.cpu(), pred_labels, average=self.config['average'])\n","        recall = recall_score(targets.cpu(), pred_labels, average=self.config['average'])\n","        return {\"val_loss\":loss, \"val_accuracy\":torch.tensor([acc]), \"val_f1\":torch.tensor([f1]), \"val_precision\":torch.tensor([precision]), \"val_recall\":torch.tensor([recall])}\n","    \n","    def validation_epoch_end(self, outputs):\n","        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n","        avg_acc = torch.stack([x['val_accuracy'] for x in outputs]).mean()\n","        avg_f1 = torch.stack([x['val_f1'] for x in outputs]).mean()\n","        avg_precision = torch.stack([x['val_precision'] for x in outputs]).mean()\n","        avg_recall = torch.stack([x['val_recall'] for x in outputs]).mean()\n","        wandb.log({\"val_loss\":avg_loss, \"val_accuracy\":avg_acc, \"val_f1\":avg_f1, \"val_precision\":avg_precision, \"val_recall\":avg_recall})\n","        return {\"val_loss\":avg_loss, \"val_accuracy\":avg_acc, \"val_f1\":avg_f1, \"val_precision\":avg_precision, \"val_recall\":avg_recall}\n","    \n","    \n","    def test_step(self, batch, batch_idx):\n","        input_ids, attention_mask, targets = batch['input_ids'], batch['attention_mask'], batch['label'].squeeze()\n","        logits = self(input_ids=input_ids, attention_mask=attention_mask)\n","        loss = F.mse_loss(logits, targets)\n","        pred_labels = logits.cpu() > 0.5 # logits.argmax(dim=1).cpu() for non-sigmoid\n","        acc = accuracy_score(targets.cpu(), pred_labels)\n","        f1 = f1_score(targets.cpu(), pred_labels, average=self.config['average'])\n","        precision = precision_score(targets.cpu(), pred_labels, average=self.config['average'])\n","        recall = recall_score(targets.cpu(), pred_labels, average=self.config['average'])\n","        return {\"test_loss\":loss, \"test_precision\":torch.tensor([precision]), \"test_recall\":torch.tensor([recall]), \"test_accuracy\":torch.tensor([acc]), \"test_f1\":torch.tensor([f1])}\n","    \n","    def test_epoch_end(self, outputs):\n","        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n","        avg_acc = torch.stack([x['test_accuracy'] for x in outputs]).mean()\n","        avg_f1 = torch.stack([x['test_f1'] for x in outputs]).mean()\n","        avg_precision = torch.stack([x['test_precision'] for x in outputs]).mean()\n","        avg_recall = torch.stack([x['test_recall'] for x in outputs]).mean()\n","        return {\"test_loss\":avg_loss, \"test_precision\":avg_precision, \"test_recall\":avg_recall, \"test_acc\":avg_acc, \"test_f1\":avg_f1}"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Training and Evaluation"]},{"cell_type":"markdown","metadata":{},"source":["##### Preprocessing\n","- csv file does not have label coloum, adding it to both csv files and save write access wokring/ directory, will read the data from here now "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["root_dir = \"../working/Fake-News/\"\n","os.makedirs(root_dir, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fake = pd.read_csv(os.path.join('../input/fake-and-real-news-dataset/', \"Fake.csv\"))\n","real = pd.read_csv(os.path.join('../input/fake-and-real-news-dataset/', \"True.csv\"))\n","\n","fake['label'] = [1]*fake.shape[0]\n","real['label'] = [0]*real.shape[0]\n","\n","fake.to_csv(os.path.join(root_dir, \"Fake.csv\"))\n","real.to_csv(os.path.join(root_dir, \"True.csv\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["config = {\n","\n","    # data \n","    \"root_dir\":root_dir,\n","    \"model_name\":\"roberta-base\",\n","    \"num_classes\":1,\n","    \"max_len\":128,\n","    \"train_size\":0.85,\n","    \"batch_size\":32,\n","    \"num_workers\":4,\n","\n","\n","\n","    # training\n","    \"average\":\"macro\",\n","    \"save_dir\":\"./\",\n","    \"project\":\"fake-news-classification\",\n","    \"lr\":2e-5,\n","    \"monitor\":\"val_accuracy\",\n","    \"min_delta\":0.005,\n","    \"patience\":2,\n","    \"filepath\":\"./checkpoints/{epoch}-{val_f1:4f}\",\n","    \"precision\":32,\n","    \"epochs\":5,\n","    \n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["### Logger, EarlyStopping and Callbacks\n","logger = WandbLogger(\n","    name=config['model_name'],\n","    save_dir=config[\"save_dir\"],\n","    project=config[\"project\"],\n","    log_model=True,\n",")\n","early_stopping = EarlyStopping(\n","    monitor=config[\"monitor\"],\n","    min_delta=config[\"min_delta\"],\n","    patience=config[\"patience\"],\n",")\n","checkpoints = ModelCheckpoint(\n","    filepath=config[\"filepath\"],\n","    monitor=config[\"monitor\"],\n","    save_top_k=1\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer = pl.Trainer(\n","    logger=logger,\n","    gpus=[0],\n","    checkpoint_callback=checkpoints,\n","    callbacks=[early_stopping],\n","    default_root_dir=\"./models/\",\n","    max_epochs=config[\"epochs\"],\n","    precision=config[\"precision\"],\n","    automatic_optimization=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dm = FakeNewsDataModule(config=config)\n","lm = LightningModel(config=config)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer.fit(\n","    model=lm,\n","    datamodule=dm\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer.test(\n","    model=lm,\n","    datamodule=dm\n",")"]},{"cell_type":"markdown","metadata":{},"source":["#### Test from Checkpoint"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_loader = dm.test_dataloader()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(os.listdir(\"../working/checkpoints/\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["l  = torch.load(f=\"./checkpoints/epoch=2-val_f1=0.999528.ckpt\")\n","lm.load_state_dict(l['state_dict'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer.test(\n","    model=lm,\n","    datamodule=dm\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### Get the pred probs and predicted labels for test set to compute other metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["actual_label = []\n","pred_label = []\n","pred_probs = []\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# [1, 2, 3] > 1"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for batch in tqdm.tqdm(test_loader):\n","    \n","    y_ = lm(input_ids=batch['input_ids'].to(device), attention_mask=batch['attention_mask'].to(device)).detach().cpu()\n","    \n","    \n","    \n","    actual_label += batch['label'].squeeze().tolist()\n","    \n","    pred_probs += y_.tolist()\n","    \n","    pred_label += y_ > 0.5\n","    \n","\n","    \n","    del batch\n","    gc.collect()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","print(classification_report(y_true=actual_label, y_pred=pred_label, digits=5))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(\"ROBERTa with MSE\")\n","print(classification_report(y_true=actual_label, y_pred=pred_label, digits=5))"]},{"cell_type":"markdown","metadata":{},"source":["## AUC ROC Curve"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["ns_probs = [0 for _ in range(len(actual_label))]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["roc_auc = roc_auc_score(y_true=actual_label, y_score=pred_probs)\n","ns_auc = roc_auc_score(y_true=actual_label, y_score=ns_probs)\n","print(f'ROC_AUC_Score = {roc_auc:.4f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lr_fpr, lr_tpr, _ = roc_curve(y_true=actual_label, y_score=pred_probs)\n","ns_fpr, ns_tpr, _ = roc_curve(y_true=actual_label, y_score=ns_probs)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# plot the roc curve for the model\n","plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n","plt.plot(lr_fpr, lr_tpr, marker='.', label='RoBERTa')\n","# axis labels\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","# show the legend\n","plt.legend()\n","# show the plot\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":4}
